---
layout: post
title: Intel - Analysis Applications
category: labnotebook
tags:
  - intel
  - hdf5
  - ff
---

# {{ page.title }}

![Analysis applications (from 2011 Peterka et al.)][ap]

# Analysis algorithm

```cpp
/**
 * Prior to the execution of the algorithm
 * data is decomposed and assigned to each
 * process (which this algorithm runs on)
 */
void ParallelAnalysisAlgorithm()
{
  ReceiveLocalChunks();

  ...


  LocalAlgorithm();

  ...

  CommunicateWithOthers();

  ...

  Output();

}
```

**Taken from 2011 Peterka et al.**

# Analysis Specification

An analysis task is specified by defining an `H5Analysis_specification` object that encapsulates the 
following:

  * Query that produces analysis script's input
  * Analysis Script
  * Parameters to script
  * TID of data being queried
  * TID of newly written data
  * Output specification

# Use Case 1: Basic Querying

> "Find all locations where pressure exceed 17 for year 2010"

```sql
FOR     '/UCAR/2010/'
WHERE   dataset_name = 'pressure' AND
        pressure > 17
```

# Use Case 2: Sorting

> "Sort the data points by pressure"

```sql
FOR      '/UCAR/2010/'
WHERE    dataset_name = 'pressure' AND
ORDER BY pressure, time
```

# Use Case 3: Per-chunk aggregation (query)

> "obtain the pressure average for chunks defined by non-overlapping \<lat,long,time,elevation> 
slabs of size \<3,4,5,6\>, ignoring values in the lat,long ranges 100-200"

```sql
FOR     '/path/to/where/dataset/is/located/'
WHERE   dataset_name = 'temperature' AND
        lat < 100 AND lat > 200
        lon < 100 AND lon > 200
```

# Use Case 3: Per-chunk aggregation (script)

~~~{.python .numberLines}
""" h5_view, mpi_comm, new_ds, objects are loaded by coordinator """

""" collectively decompose the view """
my_slabs = extract_slabs(h5_view, mpi_comm, arg_1, ...)

""" slab and new_ds are NumPy arrays (h5py datasests) """
for slab in my_slabs:
   rsum = 0

   for pressure in slab[:,:,:,:]:
      rsum += pressure

   avg = rsum / slab.attrs['num_of_elements']

   new_ds[slab.attrs['center']] = avg
~~~

# Use Case 4: Global reduction (query)

Global average across entire dataset:

> "Get average of pressure for every year in the range 2000-2009 for the points contained in 
coordinates (36,36) with elevation of 10 meters"

```sql
FOREACH DATASET IN '/UCAR/200*/*'
WHERE   dataset_name = 'pressure' AND
        lat = 36 AND
        lon = 36 AND
        elevation = 10
```

# Use Case 4: Global reduction (script)

~~~{.python .numberLines}
rsum = 0

for pressure in h5view[:,:,:,:]:
   rsum += pressure

if mpi4py.rank != 0:
   mpi4py.send(rank_0, rsum, slab.attrs['num_of_elements'])
else:
   mpi4py.gather(rsum,total)
   new_ds.attrs['avg'] = rsum / total
~~~

  * makes use of MPI communicator (`mpi4py`)
  * each process obtains local sums
  * each communicate them to rank 0

# Use Case 5: Local Neighborhood Communication

**TO-DO**: same as global reduction but selectively communicating among ranks

# Next

  * Should decomposition be part of analysis extensions?
  * `H5View`:
      * Do we need an iterator?
      * Are `H5S*` routines enough?
      * When reading the result of a query, are we expecting the user to connect individually to 
        each node and read each `H5View` independently? Do we have to aggregate views?
  * Can we code now? Possible starting points:
      * `H5AEpublic.h` API, how to make it plug-in based?
      * `H5Analysis_specification` that wraps the analytical task
      * `H5View` API

[ap]: {{ site.url }}/images/labnotebook/2013-07-30-analysis-app.png
